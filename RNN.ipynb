{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'bbdc_2019_Bewegungsdaten/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels', 'bounds', 'lens', 'label_encoder'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(data):\n",
    "    features = data['features']\n",
    "    lens = np.concatenate((data['lens']['train'],data['lens']['valid'],data['lens']['test']))\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    ret = []\n",
    "    l = 0\n",
    "    for t in lens:\n",
    "        ret.append(features[l: l + t])\n",
    "        l += t\n",
    "    features = np.array(ret)\n",
    "    data['features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, split):\n",
    "    if split == 'train':\n",
    "        return data['features'][: data['bounds']['train']]\n",
    "    elif split == 'valid':\n",
    "        return data['features'][data['bounds']['train']: data['bounds']['train'] + data['bounds']['valid']]\n",
    "    elif split == 'test':\n",
    "        return data['features'][data['bounds']['train'] + data['bounds']['valid']:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_idx(n, batch_size, randomise=False):\n",
    "    idx = np.arange(0, n)\n",
    "    if randomise:\n",
    "        np.random.shuffle(idx)\n",
    "    for batch_idx in np.arange(0, n, batch_size):\n",
    "        yield idx[batch_idx:batch_idx+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data, split, batch_size,\n",
    "                     time_steps=10, stride=5, randomise=False):\n",
    "    features = get_split(data, split)\n",
    "\n",
    "    try:\n",
    "        labels = data['label_encoder'].transform(data['labels'][split])\n",
    "    except:\n",
    "        labels = np.zeros(features.shape[0])\n",
    "    \n",
    "    lens = data['lens'][split]\n",
    "    new_features = []\n",
    "    new_labels = []\n",
    "    new_lens = []\n",
    "    for i in range(len(lens)):\n",
    "        mat = features[i]\n",
    "        label = labels[i]\n",
    "        l = lens[i]\n",
    "        for j in range(0, len(mat) - time_steps, stride):\n",
    "            new_features.append(mat[j: j + time_steps, 5:])\n",
    "            new_labels.append(label)\n",
    "            new_lens.append(i)\n",
    "    features = np.array(new_features)\n",
    "    labels = np.array(new_labels)\n",
    "    lens = np.array(new_lens)\n",
    "    \n",
    "    n = len(features)\n",
    "    for batch_idx in generate_batch_idx(n, batch_size, randomise):\n",
    "        batch_data = features[batch_idx]\n",
    "        batch_labels = labels[batch_idx]\n",
    "        batch_lens = lens[batch_idx]\n",
    "#         batch_data = torch.from_numpy(batch_data).float()\n",
    "#         batch_labels = torch.from_numpy(labels[batch_idx]).float()\n",
    "#         lens = torch.from_numpy(lens[batch_idx]).float()\n",
    "        yield batch_data, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_batch(batch, targets, lengths):\n",
    "#     \"\"\"\n",
    "#     Sort a minibatch by the length of the sequences with the longest sequences first\n",
    "#     return the sorted batch targes and sequence lengths.\n",
    "#     This way the output can be used by pack_padded_sequences(...)\n",
    "#     \"\"\"\n",
    "#     perm_idx = np.argsort(lengths)[::-1]\n",
    "#     seq_lengths = lengths[perm_idx]\n",
    "#     seq_tensor = batch[perm_idx]\n",
    "#     target_tensor = targets[perm_idx]\n",
    "#     return seq_tensor, target_tensor, seq_lengths\n",
    "\n",
    "def pad_batch(batch, lens):\n",
    "    max_len = max(lens)\n",
    "    batch_size = batch.shape[0]\n",
    "    num_feature = batch[0].shape[1]\n",
    "    padded_seqs = np.zeros((batch_size, max_len, num_feature))\n",
    "    \n",
    "    for i, l in enumerate(lens):\n",
    "        padded_seqs[i, :l, :] = batch[i][:l]\n",
    "\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_batch(batch, targets):\n",
    "    return torch.from_numpy(batch).float(), torch.from_numpy(targets).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, data, split, batch_size, time_steps, stride):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for b_data, b_labels, b_lens in generate_batches(data, split, batch_size, \n",
    "                                                         time_steps, stride, False):\n",
    "#             b_data = pad_batch(b_data, b_lens)\n",
    "            b_data, b_labels = torch_batch(b_data, b_labels)\n",
    "            preds.append(model(b_data, b_lens))\n",
    "            labels.append(b_labels)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, labels, le):\n",
    "    preds = preds.max(dim=1)[1].numpy()\n",
    "    preds = [le.classes_[i] for i in preds]\n",
    "    labels = labels.numpy()\n",
    "    labels = [le.classes_[i] for i in labels]\n",
    "    return accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HARNet(nn.Module):\n",
    "    def __init__(self, time_steps):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(14, 64)\n",
    "        self.lin2 = nn.Linear(64, 64, bias=True)\n",
    "        self.lin3 = nn.Linear(64, 22, bias=True)\n",
    "    \n",
    "    def forward(self, data, lens):\n",
    "        x, _ = self.rnn(data)\n",
    "        x = self.lin2(x[:, -1, :])\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] | Batch 7865 | Loss: 2.112215367760995: : 7865it [21:33,  3.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from inf to 2.5460805892944336\n",
      "Validation accuracy: 0.2772837202216594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 2] | Batch 7865 | Loss: 1.9484155106377767: : 7865it [21:14,  6.17it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 3] | Batch 397 | Loss: 1.942855678814287: : 397it [01:52,  8.35it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2675ebcda536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/analytics/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/analytics/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "time_steps = 200\n",
    "stride = 50\n",
    "objective = nn.CrossEntropyLoss()\n",
    "model = HARNet(time_steps)\n",
    "optimiser = torch.optim.Adam(model.parameters(), weight_decay=0.0)\n",
    "running_loss = 0\n",
    "running_batch = 0\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    with tqdm(enumerate(generate_batches(data, 'train', batch_size,\n",
    "                                         time_steps, stride, True), 1)) as pbar:\n",
    "        model.train()\n",
    "        for batch_num, (batch_data, batch_labels, batch_lens) in pbar:\n",
    "#             batch_data = pad_batch(batch_data, batch_lens)\n",
    "            batch_data, batch_labels = torch_batch(batch_data, batch_labels)\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_data, batch_lens)\n",
    "            loss = objective(preds, batch_labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item()\n",
    "            running_batch += 1\n",
    "            pbar.set_description(f'[Epoch: {epoch}] | Batch {batch_num} | Loss: {running_loss/running_batch}')\n",
    "            \n",
    "    valid_preds, valid_labels = get_preds(model, data, 'valid',\n",
    "                                          64, time_steps, stride)\n",
    "    \n",
    "    valid_loss = objective(valid_preds, valid_labels).item()\n",
    "    \n",
    "    if valid_loss < min_valid_loss:\n",
    "        print(f'Validation loss improved from {min_valid_loss} to {valid_loss}')\n",
    "        acc = get_accuracy(valid_preds, valid_labels, data['label_encoder'])\n",
    "        print(f'Validation accuracy: {acc}')\n",
    "        min_valid_loss = valid_loss\n",
    "        with open('best_rnn_model.pt', 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "    else:\n",
    "        print('Validation loss did not improve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
