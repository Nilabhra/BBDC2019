{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load('data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'bbdc_2019_Bewegungsdaten/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels', 'bounds', 'lens', 'label_encoder'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(data):\n",
    "    features = data['features']\n",
    "    lens = np.concatenate((data['lens']['train'],data['lens']['valid'],data['lens']['test']))\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    ret = []\n",
    "    l = 0\n",
    "    for t in lens:\n",
    "        ret.append(features[l: l + t])\n",
    "        l += t\n",
    "    features = np.array(ret)\n",
    "    data['features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emg(x):\n",
    "    high = 20/(1000/2)\n",
    "    low = 450/(1000/2)\n",
    "    b, a = signal.butter(4, [high, low], btype='bandpass')\n",
    "    emg_filtered = signal.filtfilt(b, a, x, axis=0)\n",
    "    return emg_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, split):\n",
    "    if split == 'train':\n",
    "        return data['features'][: data['bounds']['train']]\n",
    "    elif split == 'valid':\n",
    "        return data['features'][data['bounds']['train']: data['bounds']['train'] + data['bounds']['valid']]\n",
    "    elif split == 'test':\n",
    "        return data['features'][data['bounds']['train'] + data['bounds']['valid']:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_idx(n, batch_size, randomise=False):\n",
    "    idx = np.arange(0, n)\n",
    "    if randomise:\n",
    "        np.random.shuffle(idx)\n",
    "    for batch_idx in np.arange(0, n, batch_size):\n",
    "        yield idx[batch_idx:batch_idx+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data, split, batch_size,\n",
    "                     time_steps=10, stride=5, randomise=False):\n",
    "    features = get_split(data, split)\n",
    "\n",
    "    try:\n",
    "        labels = data['label_encoder'].transform(data['labels'][split])\n",
    "    except:\n",
    "        labels = np.zeros(features.shape[0])\n",
    "    \n",
    "    lens = data['lens'][split]\n",
    "    new_features = []\n",
    "    new_labels = []\n",
    "    new_lens = []\n",
    "    for i in range(len(lens)):\n",
    "        mat = features[i]\n",
    "        label = labels[i]\n",
    "        l = lens[i]\n",
    "        acc_emg = [0, 1, 2, 3, 5, 6, 7, 9, 10, 11]\n",
    "        mat[:, acc_emg] -= mat[:, acc_emg].mean(axis=0)\n",
    "        mat[:, :4] = filter_emg(mat[:, :4])\n",
    "        extracted_steps = []\n",
    "        for j in range(0, len(mat) - time_steps, stride):\n",
    "            window = mat[j: j + time_steps, :]\n",
    "            means = window[:, 4:].mean(axis=0).reshape(1, -1)\n",
    "            rms = np.sqrt((window[:, :4]**2).mean(axis=0)).reshape(1, -1)\n",
    "            feature_vector = np.concatenate((rms, means), axis=1).reshape(1, -1)\n",
    "            \n",
    "            extracted_steps.append(feature_vector)\n",
    "        extracted_steps = np.concatenate(extracted_steps, axis=0)\n",
    "        new_features.append(extracted_steps)\n",
    "        new_labels.append(label)\n",
    "        new_lens.append(len(extracted_steps))\n",
    "    features = np.array(new_features)\n",
    "    labels = np.array(new_labels)\n",
    "    lens = np.array(new_lens)\n",
    "    \n",
    "    n = len(features)\n",
    "    for batch_idx in generate_batch_idx(n, batch_size, randomise):\n",
    "        batch_data = features[batch_idx]\n",
    "        batch_labels = labels[batch_idx]\n",
    "        batch_lens = lens[batch_idx]\n",
    "#         batch_data = torch.from_numpy(batch_data).float()\n",
    "#         batch_labels = torch.from_numpy(labels[batch_idx]).float()\n",
    "#         lens = torch.from_numpy(lens[batch_idx]).float()\n",
    "        yield batch_data, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_batch(batch, targets, lengths):\n",
    "#     \"\"\"\n",
    "#     Sort a minibatch by the length of the sequences with the longest sequences first\n",
    "#     return the sorted batch targes and sequence lengths.\n",
    "#     This way the output can be used by pack_padded_sequences(...)\n",
    "#     \"\"\"\n",
    "#     perm_idx = np.argsort(lengths)[::-1]\n",
    "#     seq_lengths = lengths[perm_idx]\n",
    "#     seq_tensor = batch[perm_idx]\n",
    "#     target_tensor = targets[perm_idx]\n",
    "#     return seq_tensor, target_tensor, seq_lengths\n",
    "\n",
    "def pad_batch(batch, lens):\n",
    "    max_len = max(lens)\n",
    "    batch_size = batch.shape[0]\n",
    "    num_feature = batch[0].shape[1]\n",
    "    padded_seqs = np.zeros((batch_size, max_len, num_feature))\n",
    "    \n",
    "    for i, l in enumerate(lens):\n",
    "        padded_seqs[i, :l, :] = batch[i][:l]\n",
    "\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_batch(batch, targets):\n",
    "    return torch.from_numpy(batch).float(), torch.from_numpy(targets).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, data, split, batch_size, time_steps, stride):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for b_data, b_labels, b_lens in generate_batches(data, split, batch_size, \n",
    "                                                         time_steps, stride, False):\n",
    "            b_data = pad_batch(b_data, b_lens)\n",
    "            b_data, b_labels = torch_batch(b_data, b_labels)\n",
    "            preds.append(model(b_data, b_lens))\n",
    "            labels.append(b_labels)\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, labels, le):\n",
    "    preds = preds.max(dim=1)[1].numpy()\n",
    "    preds = [le.classes_[i] for i in preds]\n",
    "    labels = labels.numpy()\n",
    "    labels = [le.classes_[i] for i in labels]\n",
    "    return accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split, window, overlap):\n",
    "    features = []\n",
    "    labels = []\n",
    "    lens = []\n",
    "    for batch_data, batch_label, batch_lens in generate_batches(data, split, 1024,\n",
    "                                                                400, 200, False):\n",
    "        features.extend(batch_data)\n",
    "        labels.extend(batch_label)\n",
    "        lens.extend(batch_lens)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    lens = np.array(lens)\n",
    "    return features, labels, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_lens = get_dataset('train', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = []\n",
    "for i in range(22):\n",
    "    model = hmm.GaussianHMM(n_components=10, covariance_type='diag', n_iter=10000)\n",
    "    label = i\n",
    "    idx = (train_labels == label)\n",
    "    model.fit(np.concatenate(train_data[idx], axis=0), train_lens[idx])\n",
    "    score_list = np.array([model.score(seq) for seq in train_data[idx]])\n",
    "    scores.append({'avg_score': score_list.mean(),\n",
    "                   'std_score': score_list.std()\n",
    "                  })\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nilavro/miniconda3/envs/analytics/lib/python3.7/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    }
   ],
   "source": [
    "valid_data, valid_labels, valid_lens = get_dataset('valid', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = []\n",
    "for seq in valid_data:\n",
    "    val_scores = [model.score(seq) for model in models]\n",
    "    pred = np.argmax(val_scores)\n",
    "    valid_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92018779342723"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_labels, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nilavro/miniconda3/envs/analytics/lib/python3.7/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    }
   ],
   "source": [
    "test_data, test_labels, test_lens = get_dataset('test', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for seq in test_data:\n",
    "    test_scores = [model.score(seq) for model in models]\n",
    "    pred = np.argmax(test_scores)\n",
    "    test_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [data['label_encoder'].classes_[x] for x in test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = pd.read_csv('bbdc_2019_Bewegungsdaten/challenge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge['Label'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
