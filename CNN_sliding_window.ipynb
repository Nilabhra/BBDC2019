{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels', 'bounds', 'lens', 'label_encoder'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standard_scale(data):\n",
    "#     features = data['features']\n",
    "#     lens = np.concatenate((data['lens']['train'],data['lens']['valid'],data['lens']['test']))\n",
    "#     features = np.concatenate(features, axis=0)\n",
    "#     features = StandardScaler().fit_transform(features)\n",
    "#     ret = []\n",
    "#     l = 0\n",
    "#     for t in lens:\n",
    "#         ret.append(features[l: l + t])\n",
    "#         l += t\n",
    "#     features = np.array(ret)\n",
    "#     data['features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_emg(x):\n",
    "    high = 20/(1000/2)\n",
    "    low = 450/(1000/2)\n",
    "    b, a = signal.butter(4, [high, low], btype='bandpass')\n",
    "    emg_filtered = signal.filtfilt(b, a, x, axis=0)\n",
    "    return emg_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, split):\n",
    "    if split == 'train':\n",
    "        return data['features'][: data['bounds']['train']]\n",
    "    elif split == 'valid':\n",
    "        return data['features'][data['bounds']['train']: data['bounds']['train'] + data['bounds']['valid']]\n",
    "    elif split == 'test':\n",
    "        return data['features'][data['bounds']['train'] + data['bounds']['valid']:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_idx(n, batch_size, randomise=False):\n",
    "    idx = np.arange(0, n)\n",
    "    if randomise:\n",
    "        np.random.shuffle(idx)\n",
    "    for batch_idx in np.arange(0, n, batch_size):\n",
    "        yield idx[batch_idx:batch_idx+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data, split, batch_size,\n",
    "                     time_steps=10, stride=5, randomise=False):\n",
    "    features = get_split(data, split)\n",
    "\n",
    "    try:\n",
    "        labels = data['label_encoder'].transform(data['labels'][split])\n",
    "    except:\n",
    "        labels = np.zeros(features.shape[0])\n",
    "    \n",
    "    lens = data['lens'][split]\n",
    "    new_features = []\n",
    "    new_labels = []\n",
    "    new_lens = []\n",
    "    for i in range(len(lens)):\n",
    "        mat = features[i]\n",
    "        label = labels[i]\n",
    "        l = lens[i]\n",
    "        acc_emg = [0, 1, 2, 3, 5, 6, 7, 9, 10, 11]\n",
    "        mat[:, acc_emg] -= mat[:, acc_emg].mean(axis=0)\n",
    "        mat[:, :4] = filter_emg(mat[:, :4])\n",
    "        extracted_steps = []\n",
    "        for j in range(0, len(mat) - time_steps, stride):\n",
    "            window = mat[j: j + time_steps, :]\n",
    "            means = window[:, 4:].mean(axis=0).reshape(1, -1)\n",
    "            rms = np.sqrt((window[:, :4]**2).mean(axis=0)).reshape(1, -1)\n",
    "            feature_vector = np.concatenate((rms, means), axis=1).reshape(1, -1)\n",
    "            \n",
    "            extracted_steps.append(feature_vector)\n",
    "        extracted_steps = np.concatenate(extracted_steps, axis=0)\n",
    "        new_features.append(extracted_steps)\n",
    "        new_labels.append(label)\n",
    "        new_lens.append(len(extracted_steps))\n",
    "    features = np.array(new_features)\n",
    "    labels = np.array(new_labels)\n",
    "    lens = np.array(new_lens)\n",
    "    \n",
    "    n = len(features)\n",
    "    for batch_idx in generate_batch_idx(n, batch_size, randomise):\n",
    "        batch_data = features[batch_idx]\n",
    "        batch_labels = labels[batch_idx]\n",
    "        batch_lens = lens[batch_idx]\n",
    "#         batch_data = torch.from_numpy(batch_data).float()\n",
    "#         batch_labels = torch.from_numpy(labels[batch_idx]).float()\n",
    "#         lens = torch.from_numpy(lens[batch_idx]).float()\n",
    "        yield batch_data, batch_labels, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_batch(batch, targets, lengths):\n",
    "#     \"\"\"\n",
    "#     Sort a minibatch by the length of the sequences with the longest sequences first\n",
    "#     return the sorted batch targes and sequence lengths.\n",
    "#     This way the output can be used by pack_padded_sequences(...)\n",
    "#     \"\"\"\n",
    "#     perm_idx = np.argsort(lengths)[::-1]\n",
    "#     seq_lengths = lengths[perm_idx]\n",
    "#     seq_tensor = batch[perm_idx]\n",
    "#     target_tensor = targets[perm_idx]\n",
    "#     return seq_tensor, target_tensor, seq_lengths\n",
    "\n",
    "def pad_batch(batch, lens):\n",
    "    max_len = max(lens)\n",
    "    batch_size = batch.shape[0]\n",
    "    num_feature = batch[0].shape[1]\n",
    "    padded_seqs = np.zeros((batch_size, max_len, num_feature))\n",
    "    \n",
    "    for i, l in enumerate(lens):\n",
    "        padded_seqs[i, :l, :] = batch[i][:l]\n",
    "\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_batch(batch, targets):\n",
    "    return torch.from_numpy(batch).float(), torch.from_numpy(targets).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, data, split, batch_size, time_steps, stride):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for b_data, b_labels, b_lens in generate_batches(data, split, batch_size, \n",
    "                                                         time_steps, stride, False):\n",
    "            b_data = pad_batch(b_data, b_lens)\n",
    "            b_data, b_labels = torch_batch(b_data, b_labels)\n",
    "            preds.append(model(b_data, b_lens))\n",
    "            labels.append(b_labels)\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, labels, le):\n",
    "    preds = preds.max(dim=1)[1].numpy()\n",
    "    preds = [le.classes_[i] for i in preds]\n",
    "    labels = labels.numpy()\n",
    "    labels = [le.classes_[i] for i in labels]\n",
    "    return accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_size(a, b):\n",
    "    x = int(a/b)\n",
    "    return x*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HARNet(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(19, 64, 8)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 8)\n",
    "        self.maxpool1 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 64, 8)\n",
    "        self.conv4 = nn.Conv1d(64, 64, 4)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.lin1 = nn.Linear(64*3,22)\n",
    "        \n",
    "    def forward(self, data, lens):\n",
    "        data = data.transpose(1,2)\n",
    "        x = self.conv1(data)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat((x.mean(dim=2), x.min(dim=2)[0], x.max(dim=2)[0]), dim=1)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/nilavro/miniconda3/envs/analytics/lib/python3.7/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n",
      "[Epoch: 1] | Batch 180 | Loss: 17.206358991728887: : 180it [00:38,  4.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from inf to 2.434720754623413\n",
      "Validation accuracy: 0.20187793427230047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 2] | Batch 180 | Loss: 9.780449027816454: : 180it [00:31,  5.74it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 2.434720754623413 to 1.7874914407730103\n",
      "Validation accuracy: 0.41001564945226915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 3] | Batch 180 | Loss: 7.143953093996754: : 180it [00:31,  5.77it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.7874914407730103 to 1.450968623161316\n",
      "Validation accuracy: 0.48826291079812206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 4] | Batch 180 | Loss: 5.750286819040776: : 180it [00:32, 18.23it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.450968623161316 to 1.2132315635681152\n",
      "Validation accuracy: 0.5524256651017214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 5] | Batch 180 | Loss: 4.861990920503934: : 180it [00:30,  5.92it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.2132315635681152 to 1.058958649635315\n",
      "Validation accuracy: 0.621283255086072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 6] | Batch 180 | Loss: 4.234298718858648: : 180it [00:32,  5.62it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.058958649635315 to 0.8743835091590881\n",
      "Validation accuracy: 0.6932707355242567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 7] | Batch 180 | Loss: 3.766552388100397: : 180it [00:30,  5.98it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.8743835091590881 to 0.833899736404419\n",
      "Validation accuracy: 0.7120500782472613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 8] | Batch 180 | Loss: 3.396908714922352: : 180it [00:30, 17.73it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.833899736404419 to 0.6053511500358582\n",
      "Validation accuracy: 0.8075117370892019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 9] | Batch 180 | Loss: 3.0980246471953983: : 180it [00:34,  5.15it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.6053511500358582 to 0.556843638420105\n",
      "Validation accuracy: 0.8294209702660407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 10] | Batch 180 | Loss: 2.84658488434222: : 180it [00:41,  4.33it/s]  \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.556843638420105 to 0.5286197066307068\n",
      "Validation accuracy: 0.8419405320813772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 11] | Batch 180 | Loss: 2.6327357086945664: : 180it [00:41,  4.33it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.5286197066307068 to 0.44790413975715637\n",
      "Validation accuracy: 0.863849765258216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 12] | Batch 180 | Loss: 2.4567086263525266: : 180it [00:41, 11.46it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.44790413975715637 to 0.3938007652759552\n",
      "Validation accuracy: 0.8701095461658842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 13] | Batch 180 | Loss: 2.3033190807152506: : 180it [00:40,  4.41it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 14] | Batch 180 | Loss: 2.174044323204056: : 180it [00:41, 10.73it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.3938007652759552 to 0.3428799510002136\n",
      "Validation accuracy: 0.8904538341158059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 15] | Batch 180 | Loss: 2.0518746091453015: : 180it [00:39, 10.41it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.3428799510002136 to 0.34064343571662903\n",
      "Validation accuracy: 0.9092331768388107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 16] | Batch 180 | Loss: 1.9422575074144535: : 180it [00:39, 11.46it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.34064343571662903 to 0.305145800113678\n",
      "Validation accuracy: 0.9264475743348983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 17] | Batch 180 | Loss: 1.8457613192936952: : 180it [00:41, 13.74it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.305145800113678 to 0.2994295656681061\n",
      "Validation accuracy: 0.9280125195618153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 18] | Batch 180 | Loss: 1.7620676282149406: : 180it [00:34,  5.17it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 19] | Batch 180 | Loss: 1.685917689575002: : 180it [00:34,  5.17it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 20] | Batch 180 | Loss: 1.6206999636420774: : 180it [00:34,  5.23it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.2994295656681061 to 0.2976876199245453\n",
      "Validation accuracy: 0.917057902973396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 21] | Batch 180 | Loss: 1.5605365046413329: : 180it [00:37, 11.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 22] | Batch 180 | Loss: 1.5027406545224213: : 180it [00:36, 13.83it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 23] | Batch 180 | Loss: 1.454353124698723: : 180it [00:37,  4.85it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.2976876199245453 to 0.26435112953186035\n",
      "Validation accuracy: 0.9436619718309859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 24] | Batch 180 | Loss: 1.4047187773338348: : 180it [00:36,  4.94it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 25] | Batch 180 | Loss: 1.356790920868516: : 180it [00:33,  5.39it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 26] | Batch 180 | Loss: 1.314837270268263: : 180it [00:36, 10.75it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 27] | Batch 180 | Loss: 1.2705772586642101: : 180it [00:34,  5.18it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.26435112953186035 to 0.17565178871154785\n",
      "Validation accuracy: 0.9593114241001565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 28] | Batch 180 | Loss: 1.2285911829861265: : 180it [00:34, 11.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 29] | Batch 180 | Loss: 1.1890803130731273: : 180it [00:37,  4.86it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.17565178871154785 to 0.17179857194423676\n",
      "Validation accuracy: 0.9702660406885759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 30] | Batch 180 | Loss: 1.1524326855312124: : 180it [00:35, 12.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 0.17179857194423676 to 0.13761931657791138\n",
      "Validation accuracy: 0.9702660406885759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 31] | Batch 180 | Loss: 1.1186795531710845: : 180it [00:36,  4.95it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 32] | Batch 180 | Loss: 1.0874392524324927: : 180it [00:35,  5.11it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 33] | Batch 180 | Loss: 1.0572133641406327: : 180it [00:38, 10.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    32: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 34] | Batch 180 | Loss: 1.0275688266154972: : 180it [00:37, 11.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 35] | Batch 180 | Loss: 0.9993386528156754: : 180it [00:37,  9.38it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch: 36] | Batch 180 | Loss: 0.972782219732588: : 180it [00:40, 11.50it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ae352f6003a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                          time_steps, stride, True), 1)) as pbar:\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/analytics/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-20cf31221793>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[0;34m(data, split, batch_size, time_steps, stride, randomise)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mrms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/analytics/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "time_steps = 100\n",
    "stride = 50\n",
    "objective = nn.CrossEntropyLoss()\n",
    "model = HARNet()\n",
    "optimiser = torch.optim.Adam(model.parameters(), weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser,\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=2,\n",
    "                                                       verbose=True)\n",
    "running_loss = 0\n",
    "running_batch = 0\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    with tqdm(enumerate(generate_batches(data, 'train', batch_size,\n",
    "                                         time_steps, stride, True), 1)) as pbar:\n",
    "        model.train()\n",
    "        for batch_num, (batch_data, batch_labels, batch_lens) in pbar:\n",
    "            batch_data = pad_batch(batch_data, batch_lens)\n",
    "            batch_data, batch_labels = torch_batch(batch_data, batch_labels)\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_data, batch_lens)\n",
    "            loss = objective(preds, batch_labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item()\n",
    "            running_batch += 1\n",
    "            pbar.set_description(f'[Epoch: {epoch}] | Batch {batch_num} | Loss: {running_loss/running_batch}')\n",
    "            \n",
    "    valid_preds, valid_labels = get_preds(model, data, 'valid',\n",
    "                                          64, time_steps, stride)\n",
    "    \n",
    "    valid_loss = objective(valid_preds, valid_labels).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    if valid_loss < min_valid_loss:\n",
    "        print(f'Validation loss improved from {min_valid_loss} to {valid_loss}')\n",
    "        acc = get_accuracy(valid_preds, valid_labels, data['label_encoder'])\n",
    "        print(f'Validation accuracy: {acc}')\n",
    "        min_valid_loss = valid_loss\n",
    "        with open('best_cnn_model.pt', 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "    else:\n",
    "        print('Validation loss did not improve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_cnn_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_labels = get_preds(model, data, 'train',\n",
    "                                          64, time_steps, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04962325096130371"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = objective(train_preds, train_labels).item()\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848590323703446"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = get_accuracy(train_preds, train_labels, data['label_encoder'])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds, valid_labels = get_preds(model, data, 'valid',\n",
    "                                          64, time_steps, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1377796083688736"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss = objective(valid_preds, valid_labels).item()\n",
    "valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9702660406885759"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = get_accuracy(valid_preds, valid_labels, data['label_encoder'])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, test_labels = get_preds(model, data, 'test',\n",
    "                                          64, time_steps, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.argmax(test_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 19,  6,  ..., 12, 16, 21])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [data['label_encoder'].classes_[x] for x in test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = pd.read_csv('bbdc_2019_Bewegungsdaten/challenge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge['Label'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge.to_csv('submission4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
